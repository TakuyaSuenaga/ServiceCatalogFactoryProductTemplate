name: Upload Product Template to S3

on:
  workflow_dispatch:
    inputs:
      s3_bucket:
        description: "S3 bucket to upload to"
        required: false
      s3_key:
        description: "S3 object key or key prefix (default mirrors templates path with .zip)"
        required: false
      aws_region:
        description: "AWS region (defaults to secrets.AWS_REGION if empty)"
        required: false
  push:
    branches:
      - main
    paths:
      - templates/**
      - .github/workflows/upload-product.yml

jobs:
  detect-changes:
    name: Detect changed templates
    runs-on: ubuntu-latest
    outputs:
      files: ${{ steps.collect.outputs.files }}
      has_changes: ${{ steps.collect.outputs.has_changes }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine base and collect changes
        id: collect
        shell: bash
        run: |
          set -euo pipefail
          BASE="${{ github.event.before || '' }}"
          if [ -z "$BASE" ]; then
            if git rev-parse HEAD~1 >/dev/null 2>&1; then
              BASE="$(git rev-parse HEAD~1)"
            else
              BASE="$(git rev-parse HEAD)"
            fi
          fi
          echo "Base: $BASE"
          mapfile -t CHANGED < <(git diff --name-only "$BASE" "$GITHUB_SHA" -- 'templates/**' | grep -E '\.ya?ml$' || true)
          # Filter out deleted files
          FILTERED=()
          for f in "${CHANGED[@]}"; do
            [ -f "$f" ] && FILTERED+=("$f") || true
          done
          if [ ${#FILTERED[@]} -eq 0 ]; then
            echo "has_changes=false" >> "$GITHUB_OUTPUT"
            echo "files=[]" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          json=$(printf '%s\n' "${FILTERED[@]}" | jq -R . | jq -s . | tr -d '\n')
          echo "has_changes=true" >> "$GITHUB_OUTPUT"
          echo "files=$json" >> "$GITHUB_OUTPUT"

  package-and-upload:
    name: Package and Upload
    runs-on: ubuntu-latest
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.has_changes == 'true' }}
    permissions:
      id-token: write
      contents: read
    strategy:
      fail-fast: false
      matrix:
        file: ${{ fromJson(needs.detect-changes.outputs.files) }}
    env:
      AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
      AWS_REGION_FALLBACK: ${{ secrets.AWS_REGION }}
      S3_BUCKET_SECRET: ${{ secrets.S3_BUCKET }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ github.event.inputs.aws_region || env.AWS_REGION_FALLBACK }}

      - name: Package changed file
        run: |
          set -euo pipefail
          target="${{ matrix.file }}"
          if [ ! -f "$target" ]; then
            echo "Target not found: $target" >&2
            exit 1
          fi
          zip -j product.zip "$target"
          ls -l product.zip

      - name: Upload to S3
        env:
          INPUT_BUCKET: ${{ github.event.inputs.s3_bucket }}
          INPUT_KEY: ${{ github.event.inputs.s3_key }}
          SECRET_BUCKET: ${{ env.S3_BUCKET_SECRET }}
        run: |
          set -euo pipefail
          BUCKET="${INPUT_BUCKET:-}"; KEY="${INPUT_KEY:-}"
          if [ -z "$BUCKET" ]; then BUCKET="${SECRET_BUCKET:-}"; fi
          if [ -z "$KEY" ]; then
            # default key: use the templates/ relative directory path + product.zip
            rel_path="${{ matrix.file }}"
            rel_path="${rel_path#templates/}"
            dir_path="$(dirname "$rel_path")"
            if [ "$dir_path" = "." ]; then
              KEY="product.zip"
            else
              KEY="$dir_path/product.zip"
            fi
          fi
          if [ -z "$BUCKET" ]; then
            echo "S3 bucket is not provided. Set workflow inputs s3_bucket or secret S3_BUCKET." >&2
            exit 1
          fi
          echo "Uploading ${{ matrix.file }} as product.zip to s3://$BUCKET/$KEY"
          aws s3 cp --only-show-errors product.zip "s3://$BUCKET/$KEY"
          echo "Uploaded: s3://$BUCKET/$KEY"
